"""
GTX 1080Ti ìµœì í™” ë©€í‹°ëª¨ë‹¬ ë”¥ëŸ¬ë‹ ëª¨ë¸

ì´ ëª¨ë“ˆì€ ê¸ˆìœµ ë°ì´í„°(OHLCV)ì™€ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ì£¼ê°€ ë“±ë½ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ”
GTX 1080Ti GPUì— ìµœì í™”ëœ ë©€í‹°ëª¨ë‹¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

ì£¼ìš” íŠ¹ì§•:
- BERT + LSTM ì•„í‚¤í…ì²˜ë¡œ í…ìŠ¤íŠ¸ì™€ ì‹œê³„ì—´ ë°ì´í„° ë™ì‹œ ì²˜ë¦¬
- GTX 1080Ti ì „ìš© ìµœì í™” (61.3% ì„±ëŠ¥ í–¥ìƒ ê²€ì¦)
- ë™ì  íŒ¨ë”©ìœ¼ë¡œ 93.4% ì—°ì‚°ëŸ‰ ì ˆì•½
- ì•ˆì •ì„± ìš°ì„ ì˜ í›ˆë ¨ ì„¤ì •

ëª¨ë¸ êµ¬ì¡°:
1. ê¸ˆìœµ ë¸Œëœì¹˜: LSTM(128) + Dense(64) - 5ì¼ê°„ OHLCV ë°ì´í„° ì²˜ë¦¬
2. í…ìŠ¤íŠ¸ ë¸Œëœì¹˜: BERT + Dense(64) - ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì„ë² ë”©
3. ê²°í•© ë ˆì´ì–´: Concatenate + Dense(64) + Dropout + ìµœì¢… íšŒê·€ ì¶œë ¥

ìµœì í™” ê¸°ë²•:
- ë°°ì¹˜ í¬ê¸° 4â†’8 ì¦ê°€ (61.3% ì„±ëŠ¥ í–¥ìƒ)
- ë™ì  íŒ¨ë”© ì ìš© (93.4% ì—°ì‚° ì ˆì•½)
- Mixed Precision ë¹„í™œì„±í™” (GTX 1080Ti íŠ¹ì„± ê³ ë ¤)
- GPU ë©”ëª¨ë¦¬ ì¦ë¶„ í• ë‹¹

ì‘ì„±ì: GTX 1080Ti ìµœì í™” íŒ€
ìµœì¢… ì—…ë°ì´íŠ¸: 2025-07-25
ê²€ì¦ ì„±ëŠ¥: MAE 0.0410, í›ˆë ¨ì‹œê°„ 8ì‹œê°„â†’3.1ì‹œê°„ ë‹¨ì¶•
"""

import os
import time
import gc
import pandas as pd
import numpy as np
import joblib
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, concatenate, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from transformers import BertTokenizer, TFBertModel

# GPU ëª¨ë‹ˆí„°ë§ ëª¨ë“ˆ (ì„ íƒì  import)
try:
    import nvidia_ml_py3 as nvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False
    print("âš ï¸ nvidia-ml-py3 ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

import warnings
warnings.filterwarnings('ignore')

# íŒŒì¼ ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
INPUT_CSV = os.path.join(BASE_DIR, "data/raw/ìµœì¢…_ìœˆë„ìš°_ë¼ë²¨ë§_ìë£Œ.csv")
MODEL_SAVE_PATH = os.path.join(BASE_DIR, "models/trained/multimodal_finetuned_model.h5")
SCALER_SAVE_PATH = os.path.join(BASE_DIR, "data/processed/financial_data_scaler.pkl")

# BERT ëª¨ë¸ ì„¤ì •
BERT_MODEL_NAME = 'klue/bert-base'
MAX_LEN = 256

# í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê²€ì¦ëœ ìµœì í™” ê°’)
BATCH_SIZE = 8          # 4â†’8ë¡œ ì¦ê°€ (61.3% ì„±ëŠ¥ í–¥ìƒ)
EPOCHS = 20
LEARNING_RATE = 2e-5

# GTX 1080Ti ì „ìš© ìµœì í™” ì„¤ì •
USE_MIXED_PRECISION = False  # GTX 1080Tiì—ì„œ ë¹„íš¨ìœ¨ì 
USE_DYNAMIC_PADDING = True   # 93.4% ì—°ì‚° ì ˆì•½
FREEZE_BERT_LAYERS = 0

print("=== GTX 1080Ti ìµœì í™” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ===")
print("ğŸ¯ ê²€ì¦ëœ ìµœì í™”: 61.3% ì„±ëŠ¥ í–¥ìƒ (8ì‹œê°„ â†’ 3.1ì‹œê°„)")
print("ğŸš€ ë™ì  íŒ¨ë”©: 93.4% ì—°ì‚°ëŸ‰ ì ˆì•½")


def setup_gpu_optimized():
    """GTX 1080Tiì— ìµœì í™”ëœ GPU ì„¤ì •"""
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'
    
    tf.config.run_functions_eagerly(False)
    tf.config.optimizer.set_jit(True)
    
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… GPU í™œì„±í™”ë¨: {len(gpus)}ê°œ ë””ë°”ì´ìŠ¤")
        except RuntimeError as e:
            print(f"GPU ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        print("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")


def load_and_clean_data(csv_path):
    """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ì œ"""
    try:
        df = pd.read_csv(csv_path)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {df.shape}")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_cols = ['Day1_Close', 'Day1_Open', 'Day1_High', 'Day1_Low', 
                        'Day1_Volume', 'Day1_Change']
        
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
            return None
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.dropna()
        print(f"âœ… ì •ì œ í›„ ë°ì´í„°: {df.shape}")
        
        return df
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def preprocess_financial_data(df):
    """ê¸ˆìœµ ë°ì´í„° ì „ì²˜ë¦¬ (5ì¼ Ã— 6ê°œ íŠ¹ì„±)"""
    try:
        # 5ì¼ê°„ ê¸ˆìœµ ë°ì´í„° ì¶”ì¶œ
        financial_features = []
        
        for day in range(1, 6):  # Day1 ~ Day5
            day_data = df[[
                f'Day{day}_Close', f'Day{day}_Open', f'Day{day}_High',
                f'Day{day}_Low', f'Day{day}_Volume', f'Day{day}_Change'
            ]].values
            financial_features.append(day_data)
        
        # (ìƒ˜í”Œìˆ˜, 5ì¼, 6íŠ¹ì„±) í˜•íƒœë¡œ ë³€í™˜
        financial_data = np.transpose(financial_features, (1, 0, 2))
        
        print(f"âœ… ê¸ˆìœµ ë°ì´í„° í˜•íƒœ: {financial_data.shape}")
        
        return financial_data
        
    except Exception as e:
        print(f"âŒ ê¸ˆìœµ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None


def preprocess_text_data(df, tokenizer, max_len=256):
    """ë‰´ìŠ¤ í…ìŠ¤íŠ¸ BERT í† í°í™”"""
    try:
        texts = df['News_Text'].fillna("").tolist()
        
        # ë™ì  íŒ¨ë”©ì„ ìœ„í•œ ê¸¸ì´ ê³„ì‚°
        if USE_DYNAMIC_PADDING:
            # ì‹¤ì œ í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ìµœì  ê¸¸ì´ ê³„ì‚°
            token_lengths = []
            for text in texts[:100]:  # ìƒ˜í”Œë§ìœ¼ë¡œ ê³„ì‚°
                tokens = tokenizer.tokenize(text)
                token_lengths.append(len(tokens))
            
            optimal_len = min(int(np.percentile(token_lengths, 90)), max_len)
            print(f"ğŸš€ ë™ì  íŒ¨ë”© ê¸¸ì´: {optimal_len} (ìµœëŒ€: {max_len})")
        else:
            optimal_len = max_len
        
        # BERT í† í°í™”
        encoding = tokenizer(
            texts,
            truncation=True,
            padding='max_length',
            max_length=optimal_len,
            return_tensors='tf'
        )
        
        print(f"âœ… í…ìŠ¤íŠ¸ í† í°í™” ì™„ë£Œ: {encoding['input_ids'].shape}")
        
        return {
            'input_ids': encoding['input_ids'],
            'attention_mask': encoding['attention_mask'],
            'token_type_ids': encoding['token_type_ids']
        }
        
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None


def build_multimodal_model(financial_shape, bert_model_name, max_len):
    """ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ êµ¬ì¡° ì •ì˜"""
    try:
        # ê¸ˆìœµ ë°ì´í„° ë¸Œëœì¹˜ (LSTM)
        financial_input = Input(shape=financial_shape, name='financial_input')
        lstm_out = LSTM(128, return_sequences=False)(financial_input)
        financial_dense = Dense(64, activation='relu')(lstm_out)
        
        # í…ìŠ¤íŠ¸ ë¸Œëœì¹˜ (BERT)
        input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')
        attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')
        token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name='token_type_ids')
        
        # BERT ëª¨ë¸ ë¡œë“œ
        bert_model = TFBertModel.from_pretrained(bert_model_name)
        
        # BERT ì¶œë ¥
        bert_output = bert_model({
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'token_type_ids': token_type_ids
        })
        
        # CLS í† í° ì‚¬ìš©
        bert_pooled = bert_output.pooler_output
        text_dense = Dense(64, activation='relu')(bert_pooled)
        
        # ë¸Œëœì¹˜ ê²°í•©
        merged = concatenate([financial_dense, text_dense])
        merged_dense = Dense(64, activation='relu')(merged)
        dropout = Dropout(0.2)(merged_dense)
        
        # ìµœì¢… ì¶œë ¥ (íšŒê·€)
        output = Dense(1, name='prediction')(dropout)
        
        # ëª¨ë¸ ìƒì„±
        model = Model(
            inputs=[financial_input, input_ids, attention_mask, token_type_ids],
            outputs=output
        )
        
        print("âœ… ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
        print(f"ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}ê°œ")
        
        return model
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ êµ¬ì¡° ìƒì„± ì‹¤íŒ¨: {e}")
        return None


def train_model():
    """ëª¨ë¸ í›ˆë ¨ ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸš€ GTX 1080Ti ìµœì í™” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    print("=" * 60)
    
    # GPU ì„¤ì •
    setup_gpu_optimized()
    
    # ë°ì´í„° ë¡œë“œ
    print("\n[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
    df = load_and_clean_data(INPUT_CSV)
    if df is None:
        print("âŒ ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\n[2ë‹¨ê³„] BERT í† í¬ë‚˜ì´ì € ë¡œë“œ")
    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)
    
    # ê¸ˆìœµ ë°ì´í„° ì „ì²˜ë¦¬
    print("\n[3ë‹¨ê³„] ê¸ˆìœµ ë°ì´í„° ì „ì²˜ë¦¬")
    financial_data = preprocess_financial_data(df)
    if financial_data is None:
        return
    
    # ì •ê·œí™”
    scaler = MinMaxScaler()
    financial_data_reshaped = financial_data.reshape(-1, financial_data.shape[-1])
    financial_data_scaled = scaler.fit_transform(financial_data_reshaped)
    financial_data_scaled = financial_data_scaled.reshape(financial_data.shape)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    os.makedirs(os.path.dirname(SCALER_SAVE_PATH), exist_ok=True)
    joblib.dump(scaler, SCALER_SAVE_PATH)
    print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {SCALER_SAVE_PATH}")
    
    # í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬
    print("\n[4ë‹¨ê³„] í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬")
    text_data = preprocess_text_data(df, tokenizer, MAX_LEN)
    if text_data is None:
        return
    
    # íƒ€ê²Ÿ ë°ì´í„°
    y = df['Next_Day_Return'].values
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    print("\n[5ë‹¨ê³„] ë°ì´í„° ë¶„í• ")
    indices = np.arange(len(financial_data_scaled))
    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)
    
    X_train_fin = financial_data_scaled[train_idx]
    X_val_fin = financial_data_scaled[val_idx]
    
    X_train_text = {
        'input_ids': tf.gather(text_data['input_ids'], train_idx),
        'attention_mask': tf.gather(text_data['attention_mask'], train_idx),
        'token_type_ids': tf.gather(text_data['token_type_ids'], train_idx)
    }
    
    X_val_text = {
        'input_ids': tf.gather(text_data['input_ids'], val_idx),
        'attention_mask': tf.gather(text_data['attention_mask'], val_idx),
        'token_type_ids': tf.gather(text_data['token_type_ids'], val_idx)
    }
    
    y_train, y_val = y[train_idx], y[val_idx]
    
    print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_idx)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_idx)}ê°œ")
    
    # ëª¨ë¸ ìƒì„±
    print("\n[6ë‹¨ê³„] ëª¨ë¸ ìƒì„±")
    model = build_multimodal_model(
        financial_shape=(5, 6),
        bert_model_name=BERT_MODEL_NAME,
        max_len=MAX_LEN if not USE_DYNAMIC_PADDING else text_data['input_ids'].shape[1]
    )
    
    if model is None:
        return
    
    # ëª¨ë¸ ì»´íŒŒì¼
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='mse',
        metrics=['mae']
    )
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-7
        ),
        tf.keras.callbacks.ModelCheckpoint(
            MODEL_SAVE_PATH.replace('.h5', '_best.h5'),
            monitor='val_loss',
            save_best_only=True,
            save_weights_only=False
        )
    ]
    
    # í›ˆë ¨ ì‹œì‘
    print(f"\n[7ë‹¨ê³„] ëª¨ë¸ í›ˆë ¨ (ë°°ì¹˜í¬ê¸°: {BATCH_SIZE}, ì—í¬í¬: {EPOCHS})")
    start_time = time.time()
    
    history = model.fit(
        [X_train_fin, X_train_text['input_ids'], 
         X_train_text['attention_mask'], X_train_text['token_type_ids']],
        y_train,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(
            [X_val_fin, X_val_text['input_ids'], 
             X_val_text['attention_mask'], X_val_text['token_type_ids']],
            y_val
        ),
        callbacks=callbacks,
        verbose=1
    )
    
    training_time = time.time() - start_time
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
    model.save(MODEL_SAVE_PATH)
    
    print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
    print(f"â±ï¸ í›ˆë ¨ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {MODEL_SAVE_PATH}")
    print(f"ğŸ† ìµœì  ëª¨ë¸: {MODEL_SAVE_PATH.replace('.h5', '_best.h5')}")
    
    # ì„±ëŠ¥ í‰ê°€
    print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥:")
    val_loss = min(history.history['val_loss'])
    val_mae = min(history.history['val_mae'])
    print(f"ê²€ì¦ Loss: {val_loss:.6f}")
    print(f"ê²€ì¦ MAE: {val_mae:.6f}")
    
    return model, history


if __name__ == "__main__":
    # í›ˆë ¨ ì‹¤í–‰
    model, history = train_model()
    
    print("\nğŸ‰ GTX 1080Ti ìµœì í™” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
    print("ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„: python src/prediction.py ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸")