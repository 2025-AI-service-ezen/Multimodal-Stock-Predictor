"""
GTX 1080Ti ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì˜ˆì¸¡ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ í›ˆë ¨ëœ GTX 1080Ti ìµœì í™” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
ìƒˆë¡œìš´ ê¸ˆìœµ ë°ì´í„°ì™€ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ë¡œ ì£¼ê°€ ë“±ë½ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ìƒˆë¡œìš´ ê¸ˆìœµ ë°ì´í„° ì…ë ¥ ë° ì „ì²˜ë¦¬
2. ë‰´ìŠ¤ í…ìŠ¤íŠ¸ BERT í† í°í™”
3. í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë“±ë½ë¥  ì˜ˆì¸¡
4. ì˜ˆì¸¡ ê²°ê³¼ í•´ì„ ë° ì‹ ë¢°ë„ ë¶„ì„
5. CSV íŒŒì¼ ì¼ê´„ ì²˜ë¦¬
6. ìƒ˜í”Œ ë°ì´í„° ìë™ ìƒì„±

ì‘ì„±ì: GTX 1080Ti ìµœì í™” íŒ€
ìµœì¢… ì—…ë°ì´íŠ¸: 2025-07-25
ë²„ì „: 1.0.0
"""

import os
import sys
import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from transformers import BertTokenizer, TFBertModel
import warnings
warnings.filterwarnings('ignore')


class GTX1080TiPredictor:
    """
    GTX 1080Ti ìµœì í™” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì˜ˆì¸¡ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” í›ˆë ¨ëœ BERT+LSTM ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
    ê¸ˆìœµ ë°ì´í„°ì™€ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ë¡œë¶€í„° ì£¼ê°€ ë“±ë½ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, base_path="./"):
        """ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”"""
        self.base_path = base_path
        self.model = None
        self.scaler = None
        self.tokenizer = None
        
        # BERT ëª¨ë¸ ì„¤ì •
        self.BERT_MODEL_NAME = 'klue/bert-base'
        self.MAX_LEN = 256
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.scaler_path = os.path.join(base_path, "data/processed/financial_data_scaler.pkl")
        self.model_path = os.path.join(base_path, "models/trained/multimodal_finetuned_model_best.h5")
        self.model_path_alt = os.path.join(base_path, "models/trained/multimodal_finetuned_model.h5")
        
        print("ğŸ¯ GTX 1080Ti ë©€í‹°ëª¨ë‹¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
        print("ğŸ“Š ê¸ˆìœµ ë°ì´í„° + ë‰´ìŠ¤ í…ìŠ¤íŠ¸ â†’ ì£¼ê°€ ë“±ë½ë¥  ì˜ˆì¸¡")
        print("=" * 60)
    
    def setup_environment(self):
        """ì˜ˆì¸¡ í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ"""
        print("\n[í™˜ê²½ ì„¤ì •] GPU ë° ëª¨ë¸ ì´ˆê¸°í™”...")
        
        # 1. GPU ì„¤ì •
        self._setup_gpu()
        
        # 2. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        if not self._load_scaler():
            return False
        
        # 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
        if not self._load_tokenizer():
            return False
        
        # 4. ëª¨ë¸ ë¡œë“œ
        if not self._load_model():
            print("âš ï¸ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print("\nâœ… ì˜ˆì¸¡ í™˜ê²½ ì„¤ì • ì™„ë£Œ")
        return True
    
    def _setup_gpu(self):
        """GTX 1080Tiì— ìµœì í™”ëœ GPU ì„¤ì •"""
        try:
            gpus = tf.config.experimental.list_physical_devices('GPU')
            if gpus:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                
                print(f"âœ… GPU í™œì„±í™”: {len(gpus)}ê°œ ë””ë°”ì´ìŠ¤")
                
                for i, gpu in enumerate(gpus):
                    details = tf.config.experimental.get_device_details(gpu)
                    device_name = details.get('device_name', 'Unknown')
                    print(f"   GPU {i}: {device_name}")
            else:
                print("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                
        except RuntimeError as e:
            print(f"âš ï¸ GPU ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _load_scaler(self):
        """ê¸ˆìœµ ë°ì´í„° ì •ê·œí™”ìš© ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
        try:
            if not os.path.exists(self.scaler_path):
                print(f"âŒ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.scaler_path}")
                return False
            
            self.scaler = joblib.load(self.scaler_path)
            print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì„±ê³µ: {self.scaler_path}")
            print(f"   ì •ê·œí™” ë²”ìœ„: [{self.scaler.feature_range[0]}, {self.scaler.feature_range[1]}]")
            
            return True
            
        except Exception as e:
            print(f"âŒ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _load_tokenizer(self):
        """BERT í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            print(f"ğŸ“ BERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {self.BERT_MODEL_NAME}...")
            self.tokenizer = BertTokenizer.from_pretrained(self.BERT_MODEL_NAME)
            
            print("âœ… BERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
            print(f"   ì–´íœ˜ í¬ê¸°: {len(self.tokenizer):,}ê°œ")
            print(f"   ìµœëŒ€ ê¸¸ì´: {self.MAX_LEN} í† í°")
            
            return True
            
        except Exception as e:
            print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _load_model(self):
        """í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            # ìµœì  ëª¨ë¸ ìš°ì„  ì‹œë„
            if os.path.exists(self.model_path):
                print(f"ğŸ“ ìµœì  ëª¨ë¸ ë¡œë“œ ì‹œë„: {self.model_path}")
                self.model = tf.keras.models.load_model(self.model_path)
            elif os.path.exists(self.model_path_alt):
                print(f"ğŸ“ ëŒ€ì²´ ëª¨ë¸ ë¡œë“œ ì‹œë„: {self.model_path_alt}")
                self.model = tf.keras.models.load_model(self.model_path_alt)
            else:
                print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            print("âœ… ìµœì  ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            print(f"   ëª¨ë¸ íŒŒë¼ë¯¸í„°: {self.model.count_params():,}ê°œ")
            print(f"   ì…ë ¥ í˜•íƒœ: {[input.shape for input in self.model.inputs]}")
            print(f"   ì¶œë ¥ í˜•íƒœ: {self.model.output.shape}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, financial_data, news_text):
        """
        ë‹¨ì¼ ìƒ˜í”Œ ì˜ˆì¸¡
        
        Args:
            financial_data (np.array): 5ì¼ê°„ ê¸ˆìœµ ë°ì´í„° (5 Ã— 6)
            news_text (str): ë‰´ìŠ¤ í…ìŠ¤íŠ¸
            
        Returns:
            dict: ì˜ˆì¸¡ ê²°ê³¼
        """
        try:
            print("\n[ì˜ˆì¸¡ ìˆ˜í–‰] ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì£¼ê°€ ë“±ë½ë¥  ì˜ˆì¸¡...")
            
            # 1. ê¸ˆìœµ ë°ì´í„° ì „ì²˜ë¦¬
            print("ğŸ“Š ê¸ˆìœµ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
            financial_processed = self._preprocess_financial_data(financial_data)
            
            # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            print("ğŸ“° ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
            text_processed = self._preprocess_text_data(news_text)
            
            # 3. ì˜ˆì¸¡ ìˆ˜í–‰
            print("ğŸ”® ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
            prediction = self.model.predict([
                financial_processed['data'],
                text_processed['input_ids'],
                text_processed['attention_mask'],
                text_processed['token_type_ids']
            ], verbose=0)
            
            # 4. ê²°ê³¼ í•´ì„
            predicted_return = float(prediction[0][0])
            predicted_return_percent = predicted_return * 100
            
            # ë°©í–¥ì„± íŒë‹¨
            if predicted_return_percent > 1.0:
                direction = "ìƒìŠ¹"
            elif predicted_return_percent < -1.0:
                direction = "í•˜ë½"
            else:
                direction = "íš¡ë³´"
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ì ˆëŒ“ê°’ ê¸°ì¤€)
            abs_return = abs(predicted_return_percent)
            if abs_return > 3.0:
                confidence = "ë†’ìŒ"
            elif abs_return > 1.0:
                confidence = "ë³´í†µ"
            else:
                confidence = "ë‚®ìŒ"
            
            print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {predicted_return_percent:.2f}% ({direction})")
            
            return {
                'predicted_return': predicted_return,
                'predicted_return_percent': predicted_return_percent,
                'direction': direction,
                'confidence': confidence,
                'interpretation': self._generate_interpretation(
                    predicted_return_percent, direction, confidence
                )
            }
            
        except Exception as e:
            print(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None
    
    def _preprocess_financial_data(self, financial_data):
        """ê¸ˆìœµ ë°ì´í„° ì „ì²˜ë¦¬"""
        try:
            # í˜•íƒœ í™•ì¸
            if financial_data.shape != (5, 6):
                raise ValueError(f"ê¸ˆìœµ ë°ì´í„° í˜•íƒœê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {financial_data.shape}, ì˜ˆìƒ: (5, 6)")
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            financial_batch = financial_data.reshape(1, 5, 6)
            
            # ì •ê·œí™” (reshape for scaler)
            original_shape = financial_batch.shape
            reshaped = financial_batch.reshape(-1, financial_batch.shape[-1])
            normalized = self.scaler.transform(reshaped)
            normalized = normalized.reshape(original_shape)
            
            print(f"âœ… ê¸ˆìœµ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {normalized.shape}")
            print(f"   ì •ê·œí™” ë²”ìœ„: [{normalized.min():.3f}, {normalized.max():.3f}]")
            
            return {'data': normalized}
            
        except Exception as e:
            print(f"âŒ ê¸ˆìœµ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _preprocess_text_data(self, news_text):
        """ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        try:
            # BERT í† í°í™”
            encoding = self.tokenizer(
                news_text,
                truncation=True,
                padding='max_length',
                max_length=self.MAX_LEN,
                return_tensors='tf'
            )
            
            # ì‹¤ì œ í† í° ê¸¸ì´ ê³„ì‚°
            actual_tokens = len([t for t in self.tokenizer.tokenize(news_text) if t != '[PAD]'])
            
            print(f"âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ:")
            print(f"   ì›ë³¸ ê¸¸ì´: {len(news_text)}ì")
            print(f"   ì‹¤ì œ í† í°: {actual_tokens}ê°œ")
            print(f"   íŒ¨ë”© í›„: {self.MAX_LEN}ê°œ")
            
            return {
                'input_ids': encoding['input_ids'],
                'attention_mask': encoding['attention_mask'],
                'token_type_ids': encoding['token_type_ids']
            }
            
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_interpretation(self, predicted_return, direction, confidence):
        """ì˜ˆì¸¡ ê²°ê³¼ í•´ì„ ìƒì„±"""
        interpretations = []
        
        # ê¸°ë³¸ í•´ì„
        if direction == "ìƒìŠ¹":
            interpretations.append(f"ëª¨ë¸ì´ {predicted_return:.2f}% ìƒìŠ¹ì„ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.")
        elif direction == "í•˜ë½":
            interpretations.append(f"ëª¨ë¸ì´ {predicted_return:.2f}% í•˜ë½ì„ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.")
        else:
            interpretations.append(f"ëª¨ë¸ì´ {predicted_return:.2f}% ì†Œí­ ë³€ë™ì„ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.")
        
        # ì‹ ë¢°ë„ í•´ì„
        if confidence == "ë†’ìŒ":
            interpretations.append("ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ë†’ìŠµë‹ˆë‹¤.")
        elif confidence == "ë³´í†µ":
            interpretations.append("ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ë³´í†µì…ë‹ˆë‹¤.")
        else:
            interpretations.append("ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë¯€ë¡œ ì‹ ì¤‘í•œ íŒë‹¨ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # íˆ¬ì ì£¼ì˜ì‚¬í•­
        interpretations.append("ì´ ì˜ˆì¸¡ì€ ì°¸ê³ ìš©ì´ë©° ì‹¤ì œ íˆ¬ì ê²°ì • ì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.")
        
        return interpretations
    
    def create_sample_input(self, output_path="sample_input.csv", num_samples=3):
        """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        try:
            print("\n[ìƒ˜í”Œ ìƒì„±] í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±...")
            print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {output_path}")
            print(f"ğŸ“Š ìƒ˜í”Œ ìˆ˜: {num_samples}ê°œ")
            
            # ë””ë ‰í„°ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            samples = []
            scenarios = ["ìƒìŠ¹", "í•˜ë½", "íš¡ë³´"]
            
            for i in range(num_samples):
                scenario = scenarios[i % len(scenarios)]
                
                # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°ì´í„° ìƒì„±
                if scenario == "ìƒìŠ¹":
                    # ìƒìŠ¹ íŒ¨í„´ (5ì¼ê°„ ì ì§„ì  ìƒìŠ¹)
                    base_price = 50000
                    prices = [base_price + j * 200 + np.random.randint(-100, 100) for j in range(5)]
                    volumes = [1000000 + np.random.randint(-100000, 200000) for _ in range(5)]
                    changes = [0.004 + np.random.uniform(-0.002, 0.008) for _ in range(5)]
                    news = "ê¸°ì—… ì‹¤ì  ê°œì„  ì „ë§ ì£¼ê°€ ìƒìŠ¹ ê¸°ëŒ€ íˆ¬ìì ê´€ì‹¬ ì¦ê°€"
                    
                elif scenario == "í•˜ë½":
                    # í•˜ë½ íŒ¨í„´ (5ì¼ê°„ ì ì§„ì  í•˜ë½)
                    base_price = 52000
                    prices = [base_price - j * 300 + np.random.randint(-50, 50) for j in range(5)]
                    volumes = [1100000 + np.random.randint(-50000, 100000) for _ in range(5)]
                    changes = [-0.006 + np.random.uniform(-0.004, 0.002) for _ in range(5)]
                    news = "ì—…ê³„ ì „ë§ ë¶€ì •ì  ì‹¤ì  ìš°ë ¤ ì¦ê°€ ë§¤ë„ ì••ë ¥"
                    
                else:  # íš¡ë³´
                    # íš¡ë³´ íŒ¨í„´ (ì‘ì€ ë³€ë™)
                    base_price = 51000
                    prices = [base_price + np.random.randint(-200, 200) for _ in range(5)]
                    volumes = [1050000 + np.random.randint(-100000, 100000) for _ in range(5)]
                    changes = [np.random.uniform(-0.003, 0.003) for _ in range(5)]
                    news = "ì‹œì¥ ê´€ë§ì„¸ ì§€ì† ê±°ë˜ëŸ‰ ê°ì†Œ íš¡ë³´ ì „ë§"
                
                # CSV í–‰ ìƒì„±
                row = {}
                trend_5days = ((prices[-1] - prices[0]) / prices[0]) * 100
                
                for day in range(1, 6):
                    price = prices[day-1]
                    # OHLC ìƒì„± (Close ê¸°ì¤€ìœ¼ë¡œ ë³€ë™)
                    open_price = price + np.random.randint(-100, 100)
                    high_price = max(price, open_price) + np.random.randint(0, 150)
                    low_price = min(price, open_price) - np.random.randint(0, 150)
                    
                    row[f'day{day}_close'] = price
                    row[f'day{day}_open'] = open_price
                    row[f'day{day}_high'] = high_price
                    row[f'day{day}_low'] = low_price
                    row[f'day{day}_volume'] = volumes[day-1]
                    row[f'day{day}_change'] = changes[day-1]
                
                row['news_text'] = news
                row['scenario'] = f"{scenario} ì‹œë‚˜ë¦¬ì˜¤ (5ì¼ ì¶”ì„¸: {trend_5days:+.1f}%)"
                
                samples.append(row)
            
            # DataFrame ìƒì„± ë° ì €ì¥
            df = pd.DataFrame(samples)
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            
            print("âœ… ìƒ˜í”Œ íŒŒì¼ ìƒì„± ì™„ë£Œ")
            print("ğŸ“‹ íŒŒì¼ í˜•ì‹:")
            print("   - day1~5_close/open/high/low/volume/change: 5ì¼ê°„ ê¸ˆìœµ ë°ì´í„°")
            print("   - news_text: ê´€ë ¨ ë‰´ìŠ¤ í…ìŠ¤íŠ¸")
            print("   - scenario: ì‹œë‚˜ë¦¬ì˜¤ ì„¤ëª… (ì°¸ê³ ìš©)")
            
            print(f"\nğŸ“Š ìƒì„±ëœ ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°:")
            for i, sample in enumerate(samples):
                print(f"   ìƒ˜í”Œ {i+1}: {sample['scenario']}")
            
            return output_path
            
        except Exception as e:
            print(f"âŒ ìƒ˜í”Œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def predict_from_csv(self, input_path, output_path):
        """CSV íŒŒì¼ ì¼ê´„ ì˜ˆì¸¡"""
        try:
            print(f"\n[ì¼ê´„ ì˜ˆì¸¡] CSV íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
            print(f"ğŸ“ ì…ë ¥ íŒŒì¼: {input_path}")
            
            # ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(input_path)
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")
            
            results = []
            print("ğŸ”® ì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
            
            for idx, row in df.iterrows():
                try:
                    # ê¸ˆìœµ ë°ì´í„° ì¶”ì¶œ
                    financial_data = np.array([
                        [row['day1_close'], row['day1_open'], row['day1_high'], 
                         row['day1_low'], row['day1_volume'], row['day1_change']],
                        [row['day2_close'], row['day2_open'], row['day2_high'], 
                         row['day2_low'], row['day2_volume'], row['day2_change']],
                        [row['day3_close'], row['day3_open'], row['day3_high'], 
                         row['day3_low'], row['day3_volume'], row['day3_change']],
                        [row['day4_close'], row['day4_open'], row['day4_high'], 
                         row['day4_low'], row['day4_volume'], row['day4_change']],
                        [row['day5_close'], row['day5_open'], row['day5_high'], 
                         row['day5_low'], row['day5_volume'], row['day5_change']]
                    ])
                    
                    news_text = str(row['news_text'])
                    
                    # ì˜ˆì¸¡ ìˆ˜í–‰
                    result = self.predict(financial_data, news_text)
                    
                    if result:
                        # ê²°ê³¼ ì €ì¥
                        result_row = {
                            'sample_index': idx,
                            'predicted_return': result['predicted_return'],
                            'predicted_return_percent': result['predicted_return_percent'],
                            'direction': result['direction'],
                            'confidence': result['confidence']
                        }
                        
                        # ì¶”ê°€ ë¶„ì„ ì •ë³´
                        trend_5days = ((financial_data[-1][0] - financial_data[0][0]) / financial_data[0][0]) * 100
                        volatility = np.std([day[0] for day in financial_data]) / np.mean([day[0] for day in financial_data])
                        
                        result_row['price_trend_5days'] = trend_5days
                        result_row['volatility'] = volatility
                        result_row['has_news'] = len(news_text.strip()) > 0
                        
                        results.append(result_row)
                        
                    else:
                        # ì˜ˆì¸¡ ì‹¤íŒ¨
                        results.append({
                            'sample_index': idx,
                            'predicted_return': None,
                            'predicted_return_percent': None,
                            'direction': 'ERROR',
                            'confidence': 'ERROR'
                        })
                        
                except Exception as e:
                    print(f"âš ï¸ ìƒ˜í”Œ {idx} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    results.append({
                        'sample_index': idx,
                        'predicted_return': None,
                        'predicted_return_percent': None,
                        'direction': 'ERROR',
                        'confidence': 'ERROR'
                    })
                
                # ì§„í–‰ë¥  ì¶œë ¥
                if (idx + 1) % max(1, len(df) // 10) == 0 or idx == len(df) - 1:
                    print(f"   ì§„í–‰ë¥ : {idx + 1}/{len(df)} ({(idx + 1)/len(df)*100:.1f}%)")
            
            # ê²°ê³¼ ì €ì¥
            results_df = pd.DataFrame(results)
            results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
            
            # ê²°ê³¼ ìš”ì•½
            valid_results = results_df[results_df['direction'] != 'ERROR']
            success_rate = len(valid_results) / len(results_df) * 100
            
            print(f"\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
            print(f"   ì„±ê³µ: {len(valid_results)}ê°œ")
            print(f"   ì‹¤íŒ¨: {len(results_df) - len(valid_results)}ê°œ")
            print(f"   ì„±ê³µë¥ : {success_rate:.1f}%")
            
            if len(valid_results) > 0:
                direction_counts = valid_results['direction'].value_counts().to_dict()
                avg_return = valid_results['predicted_return_percent'].mean()
                print(f"   ë°©í–¥ ë¶„í¬: {direction_counts}")
                print(f"   í‰ê·  ì˜ˆì¸¡ ë“±ë½ë¥ : {avg_return:.2f}%")
            
            print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {output_path}")
            
            return results_df
            
        except Exception as e:
            print(f"âŒ ì¼ê´„ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None


def main():
    """ì˜ˆì¸¡ ì‹œìŠ¤í…œ ë°ëª¨ ì‹¤í–‰"""
    print("=" * 60)
    print("  GTX 1080Ti ë©€í‹°ëª¨ë‹¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("  ì£¼ê°€ ë“±ë½ë¥  ì˜ˆì¸¡ (BERT + LSTM)")
    print("=" * 60)
    
    # ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
    predictor = GTX1080TiPredictor()
    
    # í™˜ê²½ ì„¤ì •
    if not predictor.setup_environment():
        print("âŒ í™˜ê²½ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    print("\nğŸ¯ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì‚¬ìš©ë²•:")
    print("â”" * 42)
    print("1ï¸âƒ£ ë‹¨ì¼ ì˜ˆì¸¡:")
    print("   predictor.predict(financial_data, news_text)")
    print("   ")
    print("2ï¸âƒ£ ì¼ê´„ ì˜ˆì¸¡:")
    print("   predictor.predict_from_csv('input.csv', 'output.csv')")
    print("   ")
    print("3ï¸âƒ£ ìƒ˜í”Œ ìƒì„±:")
    print("   predictor.create_sample_input('sample.csv')")
    
    # ë°ëª¨ ì‹¤í–‰
    print(f"\n[ë°ëª¨] ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡...")
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    sample_path = predictor.create_sample_input("data/samples/demo_input.csv")
    
    if sample_path:
        print(f"\n[ë°ëª¨] ìƒ˜í”Œ ë°ì´í„°ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸...")
        
        # ì¼ê´„ ì˜ˆì¸¡ ìˆ˜í–‰
        results_df = predictor.predict_from_csv(
            sample_path,
            "data/samples/demo_results.csv"
        )
        
        if results_df is not None:
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            valid_results = results_df[results_df['direction'] != 'ERROR']
            
            print(f"\nğŸ“Š === ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ===")
            print(f"âœ… ì„±ê³µí•œ ì˜ˆì¸¡: {len(valid_results)}ê°œ")
            
            if len(valid_results) > 0:
                print(f"\nğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸:")
                print("-" * 50)
                
                for _, row in valid_results.iterrows():
                    direction_emoji = "ğŸ“ˆ" if row['direction'] == "ìƒìŠ¹" else "ğŸ“‰" if row['direction'] == "í•˜ë½" else "ğŸŸ¡"
                    print(f"{direction_emoji} ìƒ˜í”Œ {int(row['sample_index'])+1}: {row['direction']} {row['predicted_return_percent']:+.2f}% (ì‹ ë¢°ë„: {row['confidence']})")
                
                print(f"\nğŸ“Š í†µê³„ ìš”ì•½:")
                direction_counts = valid_results['direction'].value_counts().to_dict()
                avg_return = valid_results['predicted_return_percent'].mean()
                print(f"   í‰ê·  ì˜ˆì¸¡ ë“±ë½ë¥ : {avg_return:+.2f}%")
                print(f"   ë°©í–¥ ë¶„í¬: {direction_counts}")
    
    print(f"\nğŸ’¡ === ì‚¬ìš© ê°€ì´ë“œ ===")
    print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print(f"   - ì…ë ¥ ìƒ˜í”Œ: data/samples/demo_input.csv")
    print(f"   - ì˜ˆì¸¡ ê²°ê³¼: data/samples/demo_results.csv")
    print(f"   ")
    print(f"ğŸ”§ ì§ì ‘ ì‚¬ìš©í•˜ë ¤ë©´:")
    print(f"   1. CSV íŒŒì¼ì„ ìƒì„±í•˜ê³  í•„ìˆ˜ ì»¬ëŸ¼ì„ í¬í•¨ì‹œí‚¤ì„¸ìš”")
    print(f"   2. predict_from_csv() í•¨ìˆ˜ë¡œ ì¼ê´„ ì˜ˆì¸¡í•˜ì„¸ìš”")
    print(f"   3. ë˜ëŠ” predict() í•¨ìˆ˜ë¡œ ê°œë³„ ì˜ˆì¸¡í•˜ì„¸ìš”")
    print(f"   ")
    print(f"âš ï¸ ì£¼ì˜ì‚¬í•­:")
    print(f"   - ì´ ì˜ˆì¸¡ì€ ì°¸ê³ ìš©ì´ë©° íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤")
    print(f"   - ì‹¤ì œ íˆ¬ì ì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”")
    print(f"   - ëª¨ë¸ì˜ í•œê³„ì™€ ì‹œì¥ ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•˜ì„¸ìš”")
    
    print("=" * 60)
    print("  ì˜ˆì¸¡ ì‹œìŠ¤í…œ ë°ëª¨ ì™„ë£Œ")
    print("=" * 60)


if __name__ == "__main__":
    main()